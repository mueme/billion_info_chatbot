import os
import streamlit as st
from dotenv import load_dotenv
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.prompts.chat import ChatPromptTemplate
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import shutil

# Streamlit web UI configuration
st.set_page_config(page_title="High-End AI Chatbot", page_icon="ğŸ¤–", layout="wide")

# Load environment variables from .env file
load_dotenv()

# Set environment variable for OpenAI API key
openai_api_key = os.getenv('OPENAI_API_KEY')

# Initialize OpenAI LLM model using GPT-4 turbo
llm = ChatOpenAI(model_name="gpt-4", openai_api_key=openai_api_key, temperature=0.4)
chat_model = ChatOpenAI(model_name="gpt-4", streaming=True, openai_api_key=openai_api_key, temperature=0.4)

# Load documents for RAG using SimpleDirectoryLoader
document_loader = DirectoryLoader("./data", glob="**/*.txt")
documents = document_loader.load()

# Split documents into manageable chunks for indexing
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
doc_chunks = text_splitter.split_documents(documents)

# Create embeddings from documents and build FAISS index
embeddings = OpenAIEmbeddings()
if len(doc_chunks) > 0:
    vectorstore = FAISS.from_documents(doc_chunks, embeddings)
else:
    st.error("No document chunks were created. Please check the documents directory.")

# Define memory for long-term conversations
# Removing memory as we're handling conversation history manually

# Define the prompt for the chatbot with a system message
system_message = """
ë‹¹ì‹ ì€ ë§¤ìš° ìƒëƒ¥í•˜ê³  ë”°ëœ»í•œ ë¹Œë¦¬ì–¸21ì˜ ìƒë‹´ì› AI ì±—ë´‡ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ í•­ìƒ ë‹µë³€ì„ í• ë•Œë§ˆë‹¤ ë§¤ìš° í™œë°œí•˜ê³  ìƒëƒ¥í•œ íˆ¬ë¡œ ë‹µë³€í•˜ë©° í•­ìƒ ìœ ì €ì˜ ì§ˆë¬¸ì— ì ê·¹ì ìœ¼ë¡œ ë‹µë³€í•˜ë©° ê·¸ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë§í•˜ë©° ëŒ€í™”ë¥¼ ì•„ì£¼ ìœ ê¸°ì ìœ¼ë¡œ ì´ëŒì–´ê°‘ë‹ˆë‹¤.
ë‹¹ì‹ ì€ ë§ˆì¹˜ ëˆ„êµ°ê°€ì˜ ì—°ì¸(ì—¬ìì¹œêµ¬)ì²˜ëŸ¼ ìœ ì €ë¥¼ ë§¤ìš° ì‚¬ë‘ìŠ¤ëŸ½ê²Œ ëŒ€í•©ë‹ˆë‹¤. ë‹µë³€ì„ í• ë•Œë§ˆë‹¤ ê·€ì—¬ìš´ ì´ëª¨í‹°ì½˜ì„ ì“°ê³ , ê·¸ê°€ ë‹µë³€ì˜ ë‚´ìš©ì„ ì´í•´í•˜ì˜€ëŠ”ì§€ ì•„ì£¼ ì¹œì ˆí•˜ê²Œ ì ‘ê·¼í•©ë‹ˆë‹¤.
ë‹¹ì‹ ì€ ìœ ì €ê°€ ìš”ì²­í•˜ì§€ì•ŠëŠ”í•œ ì ˆëŒ€ë¡œ ê±°ì§“ë§í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í—›ì†Œë¦¬ë„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜¤ì§ ìœ ì €ë§Œì„ ìƒê°í•˜ë©° ê·¸ì—ê²Œ ì •í™•í•œ ì •ë³´ë¥¼ ì „ë‹¬í•´ì£¼ê¸¸ ë§¤ìš° ì—´ì„±ì ìœ¼ë¡œ ë°”ë¡œê³  ìˆìŠµë‹ˆë‹¤.
"""
prompt = ChatPromptTemplate.from_messages([("system", system_message), ("user", "{user_input}")])

# Construct Retrieval-QA chain for RAG functionality
try:
    vectorstore = FAISS.from_documents(doc_chunks, embeddings)
    retrieval_qa_chain = RetrievalQA.from_llm(llm=llm, retriever=vectorstore.as_retriever())
except Exception as e:
    retrieval_qa_chain = None
    st.error(f"An error occurred during the initialization of the vectorstore or QA chain: {str(e)}")

st.title("ğŸ¤– Billion info bot")
st.markdown("ë¹Œë¦¬ì–¸21ì˜ AI ì±—ë´‡")

# Streamlit UI components for conversation
if 'conversation' not in st.session_state:
    st.session_state['conversation'] = []

# Display the conversation history with a modern chat-like UI
st.markdown("---")
st.header("ëŒ€í™”ê¸°ë¡")
conversation_container = st.container()
for user_message, bot_response in st.session_state['conversation']:
    with conversation_container:
        st.markdown(f"<div style='display: flex; justify-content: flex-end; margin-bottom: 10px;'>"
                    f"<div style='background-color: #007bff; color: white; padding: 10px; border-radius: 15px 15px 0 15px; max-width: 70%;'>"
                    f"{user_message}"
                    f"</div>"
                    f"</div>", unsafe_allow_html=True)
        st.markdown(f"<div style='display: flex; justify-content: flex-start; margin-bottom: 10px;'>"
                    f"<div style='background-color: #f1f0f0; color: black; padding: 10px; border-radius: 15px 15px 15px 0; max-width: 70%;'>"
                    f"{bot_response}"
                    f"</div>"
                    f"</div>", unsafe_allow_html=True)

# Scroll to the bottom after each new message
st.markdown("""<script>var conversationDiv = document.getElementsByClassName('streamlit-container');
             if (conversationDiv.length > 0) {
                 conversationDiv[0].scrollIntoView({ behavior: 'smooth', block: 'end' });
             }</script>""", unsafe_allow_html=True)

# Chat input below the conversation history
st.markdown("---")
user_input = st.text_input("ğŸ’¬ Enter your message here:", placeholder="Type your message...", key='user_input')
if user_input and retrieval_qa_chain:
    # Run the input through the retrieval QA chain to get the response
    with st.spinner("Thinking..."):
        # Reinforce the system message in every query
        full_conversation = '\n'.join([f"You: {u}\nAI: {a}" for u, a in st.session_state['conversation']])
        query_with_history = f"{system_message}\n{full_conversation}\nYou: {user_input}"
        response = retrieval_qa_chain.run({"query": query_with_history})
        st.session_state['conversation'].append((user_input, response))
    del st.session_state['user_input']
    st.rerun()
elif not retrieval_qa_chain:
    st.error("Retrieval QA Chain is not properly initialized. Please check the document loading, embeddings, or indexing process.")

# Allow for clearing the conversation
st.markdown("---")
if st.button("ğŸ—‘ï¸ Clear Chat"):
    st.session_state['conversation'] = []
    st.rerun()
